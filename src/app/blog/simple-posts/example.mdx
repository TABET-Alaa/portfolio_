---
title: "Hello from the new blog"
summary: "A minimal example post in the simplified blog."
publishedAt: "2025-10-08"
tag: "Update"
---

# From Monolith to Microservices: When Configuration Turns Into Chaos ğŸŒ€

> 80% of teams migrating to microservices discover the problem **after going live**.  
> Not because of the code.  
> Not because of Kubernetes.  
> But because of **configuration**.

---

## ğŸ§© The Story

![Monolithic vs Microservices](./monolith-vs-microservices-diagram.png)

Sofia, CTO of a fast-growing e-commerce scale-up in Lyon, decided it was time for a big move.  
Their good old Java Spring monolith had served them well for five years â€” but it had become painfully slow to evolve.  

Each release took two weeks.  
Each hotfix required a full redeployment.  
So, they went for it: **migration to a microservices architecture**.  

In just three months, everything was rebuilt:  
âœ… 18 microservices containerized with Docker  
âœ… A shiny new Kubernetes cluster on AWS EKS  
âœ… Jenkins for CI/CD automation  
âœ… Separate databases on RDS  

Everything looked perfect â€” until the day of the **production update**.

---

## ğŸ’¥ The Problem

![CI/CD pipeline misconfiguration](./config-management-pipeline.png)

The team pushed a minor update to the â€œOrder Service.â€  
Just a small feature: a new â€œdelivery methodâ€ field and some logging improvements.  

But minutes later, chaos.  
Orders stopped processing.  
The â€œPaymentâ€ and â€œInventoryâ€ services went silent.  

After two stressful hours, Sofia finally found the cause:  
the updated microservice was pointing to an **old pre-production API URL**.  

Why? Because configuration values (like API URLs) were being **baked inside the Docker image** during Jenkins builds â€” one image per environment.  

Each service had its own â€œcustomâ€ build with slightly different configs.  
One small oversight, and production started talking to pre-prod.  

A textbook configuration nightmare.

---

## ğŸ“‰ The Consequences

- 4 hours of downtime for the order service  
- Over 12,000 abandoned shopping carts  
- AWS costs skyrocketing from endless redeploys  
- And worst of all â€” the team losing trust in their shiny new CI/CD pipeline  

Ironically, this â€œmodernâ€ migration was supposed to bring **more stability**, not less.

---

## ğŸ§  The Lesson

With a monolith, this setup might have worked.  
One app. One build. One config file like `application.properties`.  

But with dozens of microservices, rebuilding for every config change is **a recipe for disaster**.  

Thatâ€™s when Sofiaâ€™s team rediscovered an old truth from the **12-Factor App** methodology:  
> â€œStore configuration in the environment, not in the code.â€

![Kubernetes ConfigMap and Secrets best practice](./kubernetes-configmap.png)

Hereâ€™s how they fixed it:

- Moved secrets to **AWS Secrets Manager**  
- Used **environment variables** injected at runtime  
- Managed non-sensitive configs via **Kubernetes ConfigMaps**  
- Reused the **same Docker image** across all environments (dev, staging, prod)  
- Stopped rebuilding â€” started injecting configs dynamically  

The result?  
Faster deployments. Fewer mistakes. And a system that behaves **the same** everywhere.

---

## ğŸ’¡ Bonus: 4 Questions to Ask Before Going Microservices

1. Are my Docker images truly **immutable**?  
2. Are my configs stored **outside** the codebase?  
3. Can I deploy the **same image** to dev, staging, and prod without rebuilding?  
4. Does my CI/CD pipeline manage configs â€” or contaminate them?  

---

## âš™ï¸ Moral of the Story

In the monolith world, rebuilding per environment was â€œfine.â€  
In the microservices world, itâ€™s **operational suicide**.  

**Build once. Deploy everywhere. Inject your config.**  

Otherwise, get ready to spend your weekends debugging... `.env` files.

